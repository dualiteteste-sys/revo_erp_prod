name: DB Backup (Supabase)

on:
  workflow_dispatch:
    inputs:
      target:
        description: "Qual banco fazer backup?"
        type: choice
        required: true
        options:
          - dev
          - prod
          - verify
      mode:
        description: "Tipo de backup"
        type: choice
        required: true
        default: full
        options:
          - full
          - schema-only
      label:
        description: "Rótulo opcional (vai no nome do arquivo)"
        required: false
        default: ""
  schedule:
    # Diário 03:10 UTC (ajuste se quiser)
    - cron: "10 3 * * *"

permissions:
  contents: read

jobs:
  backup:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    env:
      R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
      # Se você criou o bucket sugerido ("revo-erp-backups-prod") e não quer manter mais uma secret,
      # esse fallback mantém o workflow funcionando.
      R2_BUCKET: ${{ secrets.R2_BUCKET || 'revo-erp-backups-prod' }}
      AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Resolve target DB URL
        id: db
        shell: bash
        env:
          INPUT_TARGET: ${{ inputs.target }}
          DEV_URL: ${{ secrets.SUPABASE_DB_URL_DEV }}
          PROD_URL: ${{ secrets.SUPABASE_DB_URL_PROD }}
          VERIFY_URL: ${{ secrets.SUPABASE_DB_URL_VERIFY }}
        run: |
          target="${INPUT_TARGET}"
          if [[ -z "$target" ]]; then
            # schedule fallback: prod
            target="prod"
          fi

          case "$target" in
            dev) url="$DEV_URL" ;;
            prod) url="$PROD_URL" ;;
            verify) url="$VERIFY_URL" ;;
            *)
              echo "Target inválido: $target"
              exit 1
              ;;
          esac

          if [[ -z "$url" ]]; then
            echo "Secret do DB URL não configurado para target=$target"
            exit 1
          fi

          echo "target=$target" >> "$GITHUB_OUTPUT"
          echo "url=$url" >> "$GITHUB_OUTPUT"

      - name: Dump
        id: dump
        shell: bash
        env:
          DATABASE_URL: ${{ steps.db.outputs.url }}
          TARGET: ${{ steps.db.outputs.target }}
          MODE: ${{ inputs.mode }}
          LABEL: ${{ inputs.label }}
        run: |
          ts="$(date -u +'%Y%m%d_%H%M%S')"
          label="${LABEL:-}"
          if [[ -n "$label" ]]; then
            label="_${label// /_}"
          fi

          file="supabase_${TARGET}_${ts}${label}.dump"
          base="supabase_${TARGET}_${ts}${label}"

          args=(--format=custom --no-owner --no-privileges)
          if [[ "$MODE" == "schema-only" ]]; then
            args+=(--schema-only)
          fi

          echo "Gerando $file (mode=$MODE)"
          # Usar pg_dump compatível com o Postgres do Supabase (evita mismatch de versão do runner).
          docker run --rm \
            -v "$PWD:/work" \
            -w /work \
            postgres:17 \
            pg_dump "${args[@]}" "$DATABASE_URL" -f "$file"

          echo "Compactando (gzip -9)..."
          gzip -9 "$file"

          gz="${file}.gz"
          sha="$(sha256sum "$gz" | awk '{print $1}')"
          size="$(stat -c '%s' "$gz" || stat -f '%z' "$gz")"

          cat > "${base}.manifest.json" <<EOF
          {
            "kind": "supabase_db_backup",
            "target": "${TARGET}",
            "mode": "${MODE}",
            "created_at_utc": "$(date -u +'%Y-%m-%dT%H:%M:%SZ')",
            "file": "${gz}",
            "sha256": "${sha}",
            "bytes": ${size},
            "git_sha": "${GITHUB_SHA}"
          }
          EOF

          ls -lh "$gz" "${base}.manifest.json"
          echo "gz=$gz" >> "$GITHUB_OUTPUT"
          echo "manifest=${base}.manifest.json" >> "$GITHUB_OUTPUT"
          echo "sha256=$sha" >> "$GITHUB_OUTPUT"
          echo "bytes=$size" >> "$GITHUB_OUTPUT"

      - name: Upload to Cloudflare R2 (durável)
        id: r2
        shell: bash
        run: |
          set -euo pipefail

          if [[ -z "${R2_ENDPOINT:-}" || -z "${R2_BUCKET:-}" || -z "${AWS_ACCESS_KEY_ID:-}" || -z "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "::warning::Secrets do R2 não configurados (R2_ENDPOINT/R2_BUCKET/R2_ACCESS_KEY_ID/R2_SECRET_ACCESS_KEY). Mantendo apenas artifact do GitHub."
            exit 0
          fi

          sudo apt-get update
          sudo apt-get install -y awscli

          target="${{ steps.db.outputs.target }}"
          prefix="revo/${target}/$(date -u +'%Y/%m/%d')"

          echo "Enviando para s3://${R2_BUCKET}/${prefix}/"

          # arquivos gerados no passo anterior:
          ls -1 supabase_${target}_*.dump.gz | head -n 50
          for f in supabase_${target}_*.dump.gz supabase_${target}_*.manifest.json; do
            if [[ ! -f "$f" ]]; then
              continue
            fi
            aws --endpoint-url "${R2_ENDPOINT}" s3 cp "$f" "s3://${R2_BUCKET}/${prefix}/${f}" --only-show-errors
          done

          # r2_key do dump (para catálogo)
          gz="${{ steps.dump.outputs.gz }}"
          echo "r2_key=${prefix}/$(basename "$gz")" >> "$GITHUB_OUTPUT"

          echo "OK: upload concluído."

      - name: Register backup in DB catalog (ops_db_backups)
        shell: bash
        env:
          DATABASE_URL: ${{ steps.db.outputs.url }}
          TARGET: ${{ steps.db.outputs.target }}
          MODE: ${{ inputs.mode }}
          R2_BUCKET: ${{ env.R2_BUCKET }}
          R2_KEY: ${{ steps.r2.outputs.r2_key }}
          SHA256: ${{ steps.dump.outputs.sha256 }}
          BYTES: ${{ steps.dump.outputs.bytes }}
          GIT_SHA: ${{ github.sha }}
        run: |
          set -euo pipefail

          if [[ -z "${R2_KEY:-}" ]]; then
            echo "::warning::R2 upload não executado; pulando registro no catálogo."
            exit 0
          fi

          echo "Registrando backup no banco (target=$TARGET)..."

          docker run --rm postgres:17 \
            psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -X -qAt -c "
              insert into public.ops_db_backups (
                target, mode, r2_bucket, r2_key, sha256, bytes, git_sha, status, meta
              ) values (
                '${TARGET}', '${MODE}', '${R2_BUCKET}', '${R2_KEY}', '${SHA256}', ${BYTES}, '${GIT_SHA}', 'uploaded',
                jsonb_build_object('source','github-actions','workflow','db-backup.yml')
              )
              on conflict (r2_key) do update
                set sha256=excluded.sha256,
                    bytes=excluded.bytes,
                    git_sha=excluded.git_sha,
                    status=excluded.status,
                    meta=excluded.meta;
            "

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: supabase-db-backup-${{ steps.db.outputs.target }}
          path: |
            *.dump.gz
            *.manifest.json
          retention-days: 7

name: Tenant Restore from R2 (Empresa) - Supabase

on:
  workflow_dispatch:
    inputs:
      target:
        description: "Qual banco restaurar?"
        type: choice
        required: true
        default: dev
        options:
          - dev
          - verify
          - prod
      empresa_id:
        description: "Empresa (tenant) id (uuid)"
        type: string
        required: true
      r2_key:
        description: "Chave completa no R2 (zip) - ex.: revo/tenants/<empresa_id>/prod/YYYY/MM/DD/tenant_export_*.zip"
        type: string
        required: true
      confirm:
        description: "Para PROD, digite RESTORE_PROD_TENANT"
        type: string
        required: false

permissions:
  contents: read
  issues: write

concurrency:
  group: tenant-restore-from-r2
  cancel-in-progress: false

jobs:
  restore:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    env:
      TARGET: ${{ inputs.target }}
      EMPRESA_ID: ${{ inputs.empresa_id }}
      R2_KEY: ${{ inputs.r2_key }}
      CONFIRM: ${{ inputs.confirm }}
      R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
      R2_BUCKET: ${{ secrets.R2_BUCKET || 'revo-erp-backups-prod' }}
      AWS_REGION: auto
      AWS_DEFAULT_REGION: auto
      AWS_EC2_METADATA_DISABLED: "true"
      AWS_S3_ADDRESSING_STYLE: path
      AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
      DEV_URL: ${{ secrets.SUPABASE_DB_URL_DEV }}
      PROD_URL: ${{ secrets.SUPABASE_DB_URL_PROD }}
      VERIFY_URL: ${{ secrets.SUPABASE_DB_URL_VERIFY }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Safety checks
        shell: bash
        run: |
          set -euo pipefail
          if [[ -z "${R2_ENDPOINT:-}" || -z "${R2_BUCKET:-}" || -z "${AWS_ACCESS_KEY_ID:-}" || -z "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "::error::Missing R2 secrets."
            exit 1
          fi
          if [[ -z "${EMPRESA_ID:-}" ]]; then
            echo "::error::Missing empresa_id."
            exit 1
          fi
          if [[ -z "${R2_KEY:-}" ]]; then
            echo "::error::Missing r2_key."
            exit 1
          fi
          if [[ "${TARGET}" == "prod" && "${CONFIRM:-}" != "RESTORE_PROD_TENANT" ]]; then
            echo "::error::Para restaurar PROD, informe confirm=RESTORE_PROD_TENANT."
            exit 1
          fi

      - name: Resolve target DB URL
        id: db
        shell: bash
        run: |
          set -euo pipefail
          case "${TARGET}" in
            dev) url="${DEV_URL}" ;;
            prod) url="${PROD_URL}" ;;
            verify) url="${VERIFY_URL}" ;;
            *) echo "::error::Target inválido: ${TARGET}"; exit 1 ;;
          esac
          if [[ -z "${url:-}" ]]; then
            echo "::error::DB URL secret ausente para target=${TARGET}"
            exit 1
          fi
          echo "url=$url" >> "$GITHUB_OUTPUT"

      - name: Install awscli + unzip
        run: |
          set -euo pipefail
          sudo apt-get update
          sudo apt-get install -y unzip python3-pip
          python3 -m pip install --upgrade pip
          python3 -m pip install --upgrade awscli
          aws configure set default.s3.addressing_style path

      - name: Install Supabase CLI (verify pre-restore)
        if: ${{ inputs.target == 'verify' }}
        uses: supabase/setup-cli@v1
        with:
          version: latest

      - name: Apply migrations to VERIFY (pre-restore)
        if: ${{ inputs.target == 'verify' }}
        env:
          DATABASE_URL: ${{ steps.db.outputs.url }}
        run: |
          set -euo pipefail
          echo "[VERIFY] Apply migrations before restore (schema must match export)..."
          echo "y" | supabase db push --include-all --db-url "$DATABASE_URL"
          docker run --rm postgres:17 \
            psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -X -qAt -c "notify pgrst, 'reload schema';"

      - name: Validate R2 access (preflight)
        shell: bash
        run: |
          set -euo pipefail
          aws --region auto --endpoint-url "${R2_ENDPOINT}" s3api head-bucket --bucket "${R2_BUCKET}"

      - name: Download zip from R2
        shell: bash
        run: |
          set -euo pipefail
          file="$(basename "${R2_KEY}")"
          echo "Baixando s3://${R2_BUCKET}/${R2_KEY}"
          aws --region auto --endpoint-url "${R2_ENDPOINT}" s3 cp "s3://${R2_BUCKET}/${R2_KEY}" "$file" --only-show-errors
          ls -lh "$file"
          unzip -q "$file" -d tenant_restore
          ls -la tenant_restore | head

      - name: Restore tenant into target DB (delete + copy)
        shell: bash
        env:
          DATABASE_URL: ${{ steps.db.outputs.url }}
        run: |
          set -euo pipefail

          # encontra o diretório exportado dentro do zip
          dir="$(find tenant_restore -maxdepth 2 -type d -name 'tenant_export_*' | head -n 1)"
          if [[ -z "${dir:-}" ]]; then
            echo "::error::Diretório tenant_export_* não encontrado no zip."
            exit 1
          fi
          tablesFile="${dir}/tables.txt"
          if [[ ! -f "$tablesFile" ]]; then
            echo "::error::tables.txt não encontrado no export."
            exit 1
          fi

          echo "Detectando tabelas do target (somente tabelas com coluna empresa_id)..."
          targetTables="target_emp_tables.txt"
          docker run --rm postgres:17 \
            psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -X -qAt -c "
              select distinct t.table_name
                from information_schema.tables t
                join information_schema.columns c
                  on c.table_schema = t.table_schema
                 and c.table_name = t.table_name
               where t.table_schema = 'public'
                 and t.table_type = 'BASE TABLE'
                 and c.column_name = 'empresa_id'
               order by 1;
            " > "$targetTables"
          sort -u "$targetTables" -o "$targetTables"

          echo "Criando script de restore..."
          restoreSql="restore_tenant.sql"
          cat > "$restoreSql" <<'SQL'
          begin;
          -- Desativa constraints/triggers temporariamente (restore por tenant).
          set session_replication_role = replica;
          SQL

          # Delete em todas as tabelas (ordem alfabética; constraints desativadas).
          while IFS= read -r t; do
            [[ -z "$t" ]] && continue
            if ! grep -Fxq "$t" "$targetTables"; then
              echo "::warning::Tabela ${t} não existe no target ou não possui empresa_id; pulando delete/import."
              continue
            fi
            echo "delete from public.\"${t}\" where empresa_id = '${EMPRESA_ID}';" >> "$restoreSql"
          done < "$tablesFile"

          echo "set session_replication_role = origin;" >> "$restoreSql"
          echo "commit;" >> "$restoreSql"

          echo "Executando deletes..."
          docker run --rm \
            -v "$PWD:/work" \
            -w /work \
            postgres:17 \
            bash -lc "psql \"${DATABASE_URL}\" -v ON_ERROR_STOP=1 -X -q -f \"${restoreSql}\""

          echo "Importando CSVs..."
          # Import por tabela (constraints/triggers desativados por conexão para evitar FK/order issues).
          while IFS= read -r t; do
            [[ -z "$t" ]] && continue
            if ! grep -Fxq "$t" "$targetTables"; then
              continue
            fi
            csv="${dir}/tables/${t}.csv"
            if [[ ! -f "$csv" ]]; then
              echo \"::warning::CSV ausente para tabela ${t}; pulando.\"
              continue
            fi

            # Se o schema do target divergir do export (muito comum em verify), ajuste o CSV
            # para bater com a contagem de colunas do target (Postgres: colunas novas normalmente
            # são adicionadas no final, então trim/pad por posição é seguro).
            tableCols="$(docker run --rm postgres:17 \
              psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -X -qAt \
                -c "select count(*) from information_schema.columns where table_schema='public' and table_name='${t}' and is_generated = 'NEVER';" \
              | tr -d '[:space:]')"
            csvCols="$(python3 scripts/tenant_csv_cols.py "$csv")"
            if [[ -n "${tableCols:-}" && -n "${csvCols:-}" && "$tableCols" != "$csvCols" ]]; then
              echo "::warning::CSV/Schema mismatch em ${t} (csv=${csvCols} cols, target=${tableCols} cols). Ajustando arquivo para restore drill..."
              mkdir -p tenant_restore/tmp
              fixed="tenant_restore/tmp/${t}.csv"
              python3 scripts/tenant_csv_pad_trim.py "$csv" "$fixed" "$tableCols"
              csv="$fixed"
            fi
            importSql="tenant_restore/tmp/import_${t}.sql"
            mkdir -p tenant_restore/tmp
            printf '%s\n' \
              "set session_replication_role=replica;" \
              "\\copy public.\"${t}\" from '/work/${csv}' with (format csv, header true);" \
              "set session_replication_role=origin;" \
              > "$importSql"
            docker run --rm \
              -v "$PWD:/work" \
              -w /work \
              postgres:17 \
              psql "${DATABASE_URL}" -v ON_ERROR_STOP=1 -X -q -f "/work/${importSql}"
          done < "$tablesFile"

      - name: Post-restore asserts (verify)
        if: ${{ inputs.target == 'verify' }}
        shell: bash
        env:
          DATABASE_URL: ${{ steps.db.outputs.url }}
        run: |
          set -euo pipefail
          echo "Running post-restore asserts for EMPRESA_ID=${EMPRESA_ID} on verify..."
          docker run --rm \
            -v "$PWD:/work" \
            -w /work \
            postgres:17 \
            bash -lc "psql \"${DATABASE_URL}\" -v ON_ERROR_STOP=1 -X -q -v empresa_id=\"${EMPRESA_ID}\" -f scripts/tenant_restore_verify_asserts.sql"

      - name: Update tenant backup catalog (status=restored)
        if: always()
        shell: bash
        env:
          DATABASE_URL: ${{ steps.db.outputs.url }}
        run: |
          set -euo pipefail
          status="restored"
          if [[ "${{ job.status }}" != "success" ]]; then status="failed"; fi

          docker run --rm postgres:17 \
            psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -X -qAt -c "
              update public.ops_tenant_backups
                 set status='${status}',
                     meta = jsonb_set(coalesce(meta,'{}'::jsonb), '{last_restore}', jsonb_build_object(
                       'at', now(),
                       'run', '${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}',
                       'target', '${TARGET}'
                     ), true)
               where r2_key = '${R2_KEY}';
            " || true

      - name: Upsert issue on failure
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            const title = "OPS ALERT: Restore de tenant falhou";
            const labels = ["ops-alert", "backup", "tenant"];
            const body = [
              "Falha ao restaurar export de tenant do R2 para o banco alvo.",
              "",
              `- target: ${process.env.TARGET}`,
              `- empresa_id: ${process.env.EMPRESA_ID}`,
              `- r2_key: ${process.env.R2_KEY}`,
              `- run: ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
            ].join("\n");

            const { data: issues } = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: "open",
              labels: labels.join(","),
              per_page: 100,
            });
            const existing = issues.find(i => i.title === title);
            if (existing) {
              await github.rest.issues.update({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: existing.number,
                body,
              });
            } else {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title,
                body,
                labels,
              });
            }
